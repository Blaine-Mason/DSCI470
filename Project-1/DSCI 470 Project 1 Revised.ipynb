{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 470 – Project 1\n",
    "\n",
    "Due: Monday, October 3rd, 11:59pm.\n",
    "\n",
    "In this project you will use prediction models to study house prices in Salt Lake City. We will use classification models to study the popularity of news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Data\n",
    "Fill out the following information: \n",
    "\n",
    "*First Name: Blaine*   \n",
    "*Last Name: Mason*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1a: Gradient Descent\n",
    "Using the mglearn make wave function with 100 samples, create a linear regression model $f$ using gradient descent that predicts the target value given the single feature. Your model should be of the form \n",
    "\n",
    "$$\n",
    "f_{w,b}(\\text{target}) = w*x+b, \n",
    "$$\n",
    "where $x$ is feature. \n",
    "\n",
    "Remeber Gradient Descent utilizes the least squares cost function:\n",
    "\n",
    "$$\n",
    "J(w,b)=\\frac{1}{2m} \\sum_{i=1}^m (f_{w,b}(x_i)-y_i)^2\n",
    "$$\n",
    "\n",
    "and we want to update:\n",
    "\n",
    "$$\n",
    "w=w-\\alpha \\frac{\\partial}{\\partial w}J(w,b)\n",
    "$$\n",
    "$$\n",
    "b=b-\\alpha \\frac{\\partial}{\\partial b}J(w,b)\n",
    "$$\n",
    "**Question 1**: intilize your Gradient Descent algortihm. Choose a few different alpha values and use a countour diagram along with the parameters from each step of the algorithm to visualize what happens when the alpha value is to big or to small\n",
    "\n",
    "**Question 2**: Pick the alpha value that gets you closest to the minimum in the fewest steps. Plot a scatter plot of the data and your model. Calculate the R squared value and report your interpertation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:0, loss: 101765.000000\n",
      "i:1, loss: 85543.286021\n",
      "i:2, loss: 72082.741480\n",
      "i:3, loss: 60905.108093\n",
      "i:4, loss: 51616.426885\n",
      "i:5, loss: 43891.924677\n",
      "i:6, loss: 37463.655304\n",
      "i:7, loss: 32110.385277\n",
      "i:8, loss: 27649.309640\n",
      "i:9, loss: 23929.261454\n",
      "i:10, loss: 20825.141235\n",
      "i:11, loss: 18233.343620\n",
      "i:12, loss: 16067.999830\n",
      "i:13, loss: 14257.888060\n",
      "i:14, loss: 12743.891134\n",
      "i:15, loss: 11476.902907\n",
      "i:16, loss: 10416.102927\n",
      "i:17, loss: 9527.533523\n",
      "i:18, loss: 8782.925434\n",
      "i:19, loss: 8158.727867\n",
      "i:20, loss: 7635.306801\n",
      "i:21, loss: 7196.281882\n",
      "i:22, loss: 6827.977543\n",
      "i:23, loss: 6518.968351\n",
      "i:24, loss: 6259.702138\n",
      "i:25, loss: 6042.187365\n",
      "i:26, loss: 5859.733615\n",
      "i:27, loss: 5706.736000\n",
      "i:28, loss: 5578.495951\n",
      "i:29, loss: 5471.072125\n",
      "i:30, loss: 5381.156288\n",
      "i:31, loss: 5305.969916\n",
      "i:32, loss: 5243.177990\n",
      "i:33, loss: 5190.817082\n",
      "i:34, loss: 5147.235314\n",
      "i:35, loss: 5111.042193\n",
      "i:36, loss: 5081.066677\n",
      "i:37, loss: 5056.322085\n",
      "i:38, loss: 5035.976722\n",
      "i:39, loss: 5019.329269\n",
      "i:40, loss: 5005.788143\n",
      "i:41, loss: 4994.854187\n",
      "i:42, loss: 4986.106136\n",
      "i:43, loss: 4979.188403\n",
      "i:44, loss: 4973.800812\n",
      "i:45, loss: 4969.689961\n",
      "i:46, loss: 4966.641949\n",
      "i:47, loss: 4964.476254\n",
      "i:48, loss: 4963.040563\n",
      "i:49, loss: 4962.206421\n",
      "i:50, loss: 4961.865556\n",
      "i:51, loss: 4961.926770\n",
      "i:52, loss: 4962.313315\n",
      "i:53, loss: 4962.960682\n",
      "i:54, loss: 4963.814717\n",
      "i:55, loss: 4964.830035\n",
      "i:56, loss: 4965.968676\n",
      "i:57, loss: 4967.198966\n",
      "i:58, loss: 4968.494547\n",
      "i:59, loss: 4969.833564\n",
      "i:60, loss: 4971.197965\n",
      "i:61, loss: 4972.572917\n",
      "i:62, loss: 4973.946299\n",
      "i:63, loss: 4975.308285\n",
      "i:64, loss: 4976.650976\n",
      "i:65, loss: 4977.968101\n",
      "i:66, loss: 4979.254752\n",
      "i:67, loss: 4980.507166\n",
      "i:68, loss: 4981.722537\n",
      "i:69, loss: 4982.898856\n",
      "i:70, loss: 4984.034780\n",
      "i:71, loss: 4985.129511\n",
      "i:72, loss: 4986.182708\n",
      "i:73, loss: 4987.194397\n",
      "i:74, loss: 4988.164906\n",
      "i:75, loss: 4989.094806\n",
      "i:76, loss: 4989.984861\n",
      "i:77, loss: 4990.835985\n",
      "i:78, loss: 4991.649211\n",
      "i:79, loss: 4992.425658\n",
      "i:80, loss: 4993.166508\n",
      "i:81, loss: 4993.872986\n",
      "i:82, loss: 4994.546341\n",
      "i:83, loss: 4995.187835\n",
      "i:84, loss: 4995.798730\n",
      "i:85, loss: 4996.380278\n",
      "i:86, loss: 4996.933716\n",
      "i:87, loss: 4997.460256\n",
      "i:88, loss: 4997.961085\n",
      "i:89, loss: 4998.437356\n",
      "i:90, loss: 4998.890189\n",
      "i:91, loss: 4999.320670\n",
      "i:92, loss: 4999.729846\n",
      "i:93, loss: 5000.118725\n",
      "i:94, loss: 5000.488279\n",
      "i:95, loss: 5000.839438\n",
      "i:96, loss: 5001.173098\n",
      "i:97, loss: 5001.490113\n",
      "i:98, loss: 5001.791302\n",
      "i:99, loss: 5002.077448\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def Cost(w,b,x,y):\n",
    "    # We take in a w,b value and a vector of features x and the target y#\n",
    "    \n",
    "    C=.5*np.sum(((w*x+b)-y)**2)\n",
    "    \n",
    "    return C\n",
    "def f(x,w,b):\n",
    "    return w*x + b\n",
    "\n",
    "# def step_gradient(b_current, m_current, points, learningRate):\n",
    "#     b_gradient = 0\n",
    "#     m_gradient = 0\n",
    "#     N = float(len(points))\n",
    "#     for i in range(0, len(points)):\n",
    "#         x = points[i, 0]\n",
    "#         y = points[i, 1]\n",
    "#         b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "#         m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "#     new_b = b_current - (learningRate * b_gradient)\n",
    "#     new_m = m_current - (learningRate * m_gradient)\n",
    "#     return [new_b, new_m]\n",
    "\n",
    "\n",
    "\n",
    "def gradientDescent(x, y, theta, learn_rate, N, n_iter):\n",
    "    loss_i = np.zeros(n_iter)\n",
    "    for i in range(n_iter):\n",
    "        w = theta[0]\n",
    "        b = theta[1]\n",
    "        yhat = w*x+b\n",
    "        loss = np.sum((yhat-y)** 2)/(2*N)\n",
    "        loss_i[i] = loss\n",
    "        print(\"i:%d, loss: %f\" % (i, loss))\n",
    "\n",
    "        gradient_w = np.dot(x,(yhat-y))/N\n",
    "        gradient_b = np.sum((yhat-y))/N\n",
    "        w = w - learn_rate*gradient_w\n",
    "        b = b - learn_rate*gradient_b\n",
    "        theta = [w,b]\n",
    "    return theta,loss_i\n",
    "\n",
    "w,b = gradientDescent(x.T,y,np.zeros(2),.0001, x.shape[0],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\" \n",
    "_df=pd.read_csv(url, header=None)\n",
    "_df[1:10]\n",
    "d = _df.values\n",
    "\n",
    "\n",
    "x=_df[23].to_numpy()\n",
    "x=np.reshape(x, (len(x), 1))\n",
    "\n",
    "\n",
    "\n",
    "y=_df[24].to_numpy()\n",
    "y=np.reshape(y, (len(y), 1))\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel(\"City\");\n",
    "plt.ylabel(\"Highway\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1b: Regression of real estate data\n",
    "For this problem, you will analyze SLC real estate data. The dataset contains multiple listing service (MLS) real estate transactions for houses sold in 2015-16 in zip code 84103 ([SLC avenues neighborhood](https://www.google.com/maps/place/Salt+Lake+City,+UT+84103/@40.8030372,-111.8957957,12z/data=!3m1!4b1!4m5!3m4!1s0x87525f672006dded:0x311e638d9a1a2de5!8m2!3d40.810506!4d-111.8449346)). We are primarily interested in regressing the `SoldPrice` on the house attributes (`property size`, `house size`, `number of bedrooms`, etc...). \n",
    "\n",
    "\n",
    "### Task 1.1: Import the data \n",
    "Use the [`pandas.read_csv()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) function to import the dataset. The data is contained in two files located in the same directory as this notebook: [`train1.csv`](train1.csv) and [`train2.csv`](train2.csv). After you import these files separately, concatenate them into one big dataframe. This pandas dataframe will be used for data exploration and linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup \n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# imports ski-kit modules\n",
    "from sklearn import tree, svm, metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#%matplotlib notebook\n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (10, 6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Clean the data \n",
    "\n",
    "1. There are 206 different variables associated with each of the 348 houses in this dataset. Skim them and try to get a rough understanding of what information this dataset contains. If you've never seen a real estate listing before, you might take a look at one on [this](http://www.utahrealestate.com/) website to get a better sense of the meanings of the column headers in the dataset.  \n",
    "\n",
    "+ Only keep houses with List Price between 250,000 and 1,000,000 dollars. This is an arbitrary choice and we realize that some people are high rollers, but for our purposes we'll consider the others as outliers. \n",
    "\n",
    "+ Remove columns that you don't think contribute to the value of the house. This is a personal decision – what attributes of a house are important to you? \n",
    "You should at least keep the following variables since the questions below will use them: \n",
    "`['Acres', 'Deck', 'GaragCap', 'Latitude', 'Longitude', 'DaysOnMkt', 'LstPrice', 'Patio', 'PkgSpacs', 'PropType', 'SoldPrice', 'Taxes', 'TotBed', 'TotBth', 'TotSqf', 'YearBlt']` \n",
    "\n",
    "+ Check the datatypes and convert any numbers that were read as strings to numerical values. (Hint: You can use [`str.replace()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.replace.html) to work with strings.) If there are any categorical values you're interested in, then you should convert them to numerical values. In particular, convert 'TotSqf' to an integer and add a column titled `Prop_Type_num` that is \n",
    "$$\n",
    "\\text{Prop_Type_num}_i = \\begin{cases} \n",
    "0 & \\text{if $i$-th listing is a condo or townhouse} \\\\\n",
    "1 & \\text{if $i$-th listing is a single family house}\n",
    "\\end{cases}. \n",
    "$$\n",
    "+ Remove the listings with erroneous `Longitude` (one has Longitude = 0) and `Taxes` values (two have unreasonably large values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Exploratory data analysis \n",
    "\n",
    "1. Explore the dataset. Write a short description of the dataset describing the number of items, the number of variables and check to see if the values are reasonable. \n",
    "\n",
    "+ Make a bar chart showing the breakdown of the different types of houses (single family, townhouse, condo). \n",
    "\n",
    "+ Compute the correlation matrix and use a heat map to visualize the correlation coefficients. \n",
    "    - Use a diverging color scale from -1 to +1 (see `vmin` and `vmax` parameters for [pcolor](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.pcolor.html))\n",
    "    - Show a legend\n",
    "    - Make sure the proper labels are visible and readable (see [`xticks`](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.xticks.html) and the corresponding [`yticks`](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.yticks.html).\n",
    "\n",
    "+ Make a scatter plot matrix to visualize the correlations. Color-code the dots by property type. For the plot, only use a subset of the columns: `['Acres', 'LstPrice', 'SoldPrice', 'Taxes', 'TotBed', 'TotBth', 'TotSqf', 'YearBlt']`. Determine which columns have strong correlations. \n",
    "\n",
    "+ Describing your findings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4: Geospatial plot\n",
    "Two of the variables are the latitude and longitude of each listing. Salt Lake City is on this nice east-west, north south grid, so even a simple plot of lat and long makes sense. Create a scatterplot of these two variables. Use color to indicate the price of the house. How does the price depend on the house location?\n",
    "\n",
    "What can you say about the relation between the location and the house price?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.5: Machine Learning K-Nearest Neighbors (KNN)\n",
    "\n",
    "Develop a k-NN regression model for predicting selling price from Taxes. Use cross validation to choose the best value of k. What is the best accuracy you can obtain on the test data? Plot graph of accuracy with various values of k to show your result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpertation** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 1.6: Machine Learning Linear Regression \n",
    "\n",
    "Use the linear regression model from the sklearn package to regress the Sold price based on the Taxes. Your model should be of the form:\n",
    "$$\n",
    "\\text{Sold Price} = \\beta_0 + \\beta_1 x, \n",
    "$$\n",
    "where $x$ is the Taxes for the property. \n",
    "\n",
    "Report the R-squared value for this model (`SoldPrice ~ Taxes`) and give an interpretation for its meaning. Also give an interpretation of $\\beta_1$ for this model. Make a scatterplot of Taxes vs. sold price and overlay the prediction coming from your regression model. Which model preforms better KNN or regression? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.7: Multilinear Regression \n",
    "Develop a multilinear regression model for house prices in this neighborhood. We could use this to come up with a list price for houses coming on the market, so do not include the list price in your model and, for now, ignore the categorical variable Prop_Type. Your model should be of the form:\n",
    "$$\n",
    "\\text{Sold Price} = b + w_1 x_1 + w_2 x_2 + \\cdots +  w_n x_n, \n",
    "$$\n",
    "where $x_i$ are predictive variables.\n",
    "\n",
    "\n",
    "**Question 1**: If we wanted to start a 'house flipping' company, we'd have to be able to do a better job of predicting the sold price than the list price does. How does your model compare? \n",
    "\n",
    "Next look at the difference between list price and sold price explicitly. Calculate two new columns for your dataset. `DiffPriceAbsolute` and `DiffPriceRelative`.\n",
    "\n",
    "* `DiffPriceAbsolute` - This is difference between sold price and list price. If it is positive, that means the house sold for more than it was listed at.\n",
    "* `DiffPriceRelative` - This is the relative difference between sold price and list price. A value of 1.1 here means that the house sold for 110% of the asking price, and 0.9 means the house sold for 90% of the asking price.\n",
    "\n",
    "Now, create two new models. One to predict `DiffPriceAbsolute`, and one to predict `DiffPriceRelative`. Use the same predictive variables as in the last model.\n",
    "\n",
    "\n",
    "**Question 2**: Which of these two new models makes better predictions.\n",
    "\n",
    "**Question 3**: Based on your answer to question two, why are these models different/the same.\n",
    "\n",
    "To help justify your answer to question 3, train two models to predict `DiffPriceAbsolute` and `DiffPriceRelative` based on just `SoldPrice`. In addition, for each model make a scatterplots similar to Task 6 for these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation:** TODO - answer Questions 1, 2, and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.8: Incorporating a categorical variable\n",
    "\n",
    "Above, we considered houses, townhouses, and condos together, but here we'll distinguish between them. Consider the two regression models: \n",
    "$$\n",
    "\\text{SoldPrice} = \\beta_0 + \\beta_1 \\text{Prop_Type_num}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\text{SoldPrice} = \\beta_0  + \\beta_1 \\text{Prop_Type_num} + \\beta_2 \\text{TotSqf}\n",
    "$$\n",
    "\n",
    "Explian the difference between the two models and if the catgorical variable gives you any additional predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Popularity of online news\n",
    "\n",
    "For this problem, you will use classification tools to predict the popularity of online news based on attributes such as the length of the article, the number of images, the day of the week that the article was published, and some variables related to the content of the article. You can learn details about the datasetat the\n",
    "[UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity). \n",
    "This dataset was first used in the following conference paper: \n",
    "\n",
    "K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. *Proceedings of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence* (2015).\n",
    "\n",
    "The dataset contains variables describing 39,644 articles published between January 7, 2013 and Januyary 7, 2015 on the news website, [Mashable](http://mashable.com/). \n",
    "There are 61 variables associated with each article. Of these, 58 are *predictor* variables, 2 are variables that we will not use (url and timedelta), and finally the number of shares of each article. The number of shares is what we will use to define whether or not the article was *popular*, which is what we will try to predict. You should read about the predictor variables in the file *OnlineNewsPopularity.names*. Further details about the collection and processing of the articles can be found in the conference paper. \n",
    "\n",
    "\n",
    "### Task 2.1 Import the data \n",
    "* Use the pandas.read_csv() function to import the dataset. Then, **print** shape of the data and first few lines of data.\n",
    "* To use [scikit-learn](http://scikit-learn.org), we'll need to save the data as a numpy array. Use the *DataFrame.values* command to export the predictor variables as a numpy array called *X* this array should not include our target variable (the number of shares). We don't need the url and timedelta, so let's drop these columns. \n",
    "* Export the number of shares as a separate numpy array, called *shares*. We'll define an article to be popular if it received more shares than the median number of shares. Create a binary numpy array, *y*, which indicates whether or not each article is popular.\n",
    "* Print *y* array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Exploratory data analysis \n",
    "\n",
    "First check to see if the values are reasonable. What are the min, median, and maximum number of shares? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 Classification using k-NN\n",
    "Develop a k-NN classification model for the data. Use cross validation to choose the best value of k. What is the best accuracy you can obtain on the test data? Plot graph of accuracy with various values of k to show your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4 Classification using SVM\n",
    "Develop a support vector machine classification model for the data.Show the results of cross-validation along with best parameter at the end.\n",
    "\n",
    "Hint: SVM is more computationally expensive, so you might want to start by using only a fraction of the data, say 4,000 articles. It takes multiple minutes to run on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5 Classification using Multinomial Logistic Regression\n",
    "\n",
    "Develop a logistic regression model for the data. Show the results for different penalty wieghts along with the best weight for the penalty at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5 Classification using decision trees\n",
    "\n",
    "Develop a decision tree classification model for the data. Use cross validation to choose good values of the max tree depth (*max_depth*) and minimum samples split (*min_samples_split*). \n",
    "Show the results of cross-validation along with best parameter at the end.\n",
    "* we don't need a plot here, we only need cross validation output and the optimal setting of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.6 Describe your findings\n",
    "\n",
    "1. Which method (k-NN, SVM, Multinomial Logistic Regression, Decision Tree) worked best?\n",
    "+ How did different parameters influence the accuracy?\n",
    "+ Which model is easiest do interpret?\n",
    "+ How would you interpret your results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation:** TODO"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
