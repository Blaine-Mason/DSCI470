{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "52521e43-7770-4068-9f69-70901926070b",
      "metadata": {
        "id": "52521e43-7770-4068-9f69-70901926070b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "MAX_TIME_STEP = 30"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V4GHHYFc1TkS",
        "outputId": "6c761022-3848-49e4-c18f-54da3c0fd141",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "V4GHHYFc1TkS",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "132d2036-fab5-4318-9102-94fb2020f4b1",
      "metadata": {
        "id": "132d2036-fab5-4318-9102-94fb2020f4b1",
        "outputId": "64407acd-e1af-4108-ad79-11c4ecc3251e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6384\n"
          ]
        }
      ],
      "source": [
        "df_raw = pd.read_csv(\"/content/drive/MyDrive/data/archive(1)/LeagueofLegends.csv\")\n",
        "df_raw = df_raw[df_raw['gamelength'] >= MAX_TIME_STEP]\n",
        "df_raw.reset_index(drop = True, inplace = True)\n",
        "print(len(df_raw))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "a1ad73e9-8f09-42f9-960d-680d12fd6055",
      "metadata": {
        "id": "a1ad73e9-8f09-42f9-960d-680d12fd6055",
        "outputId": "0c80ecac-324d-4f89-f192-d203ec1d7ae6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6384\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/data/archive(1)/LeagueofLegends.csv\")\n",
        "df = df[df['gamelength'] >= MAX_TIME_STEP]\n",
        "df.reset_index(drop = True, inplace = True)\n",
        "matches = len(df)\n",
        "print(matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "c3d76baf-f578-4708-8f90-3afe59ae9ba2",
      "metadata": {
        "id": "c3d76baf-f578-4708-8f90-3afe59ae9ba2"
      },
      "outputs": [],
      "source": [
        "def count_item(items):\n",
        "    count = np.zeros(MAX_TIME_STEP, dtype=np.int8)\n",
        "    for timestep in range(MAX_TIME_STEP) :\n",
        "        for item in items:\n",
        "            if item[0] <= timestep + 1:\n",
        "                count[timestep] += 1\n",
        "    return count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "7fa85187-eb0d-4f84-a5f5-4844d2609f81",
      "metadata": {
        "id": "7fa85187-eb0d-4f84-a5f5-4844d2609f81"
      },
      "outputs": [],
      "source": [
        "from ast import literal_eval\n",
        "df['golddiff'] = df['golddiff'].apply(literal_eval)\n",
        "blue = ['bDragons', 'bBarons', 'bHeralds', 'bTowers', 'bInhibs', 'bKills']\n",
        "red = ['rDragons', 'rBarons', 'rHeralds', 'rTowers', 'rInhibs', 'rKills']\n",
        "diffs = ['dragondiff', 'barondiff', 'heralddiff', 'towerdiff', 'inhibitordiff', 'killdiff']\n",
        "for r, b, d in zip(blue, red, diffs):\n",
        "    df[b] = df[b].apply(literal_eval)\n",
        "    df[r] = df[r].apply(literal_eval)\n",
        "\n",
        "    df[b] = df[b].apply(count_item)\n",
        "    df[r] = df[r].apply(count_item)\n",
        "    df[d] = df[b] - df[r]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "c8f5e620-6f7f-462a-8dcf-05cd705a6b6a",
      "metadata": {
        "id": "c8f5e620-6f7f-462a-8dcf-05cd705a6b6a"
      },
      "outputs": [],
      "source": [
        "df_raw[\"bKills\"] = df_raw[\"bKills\"].apply(literal_eval)\n",
        "df_raw[\"rKills\"] = df_raw[\"rKills\"].apply(literal_eval)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bkls = df_raw[\"bKills\"]\n",
        "btags = df_raw[\"blueTeamTag\"]\n",
        "bjng = df_raw[\"blueJungle\"]\n",
        "ganks_wtime = []\n",
        "for jnglr, res, tag in zip(bjng, bkls, btags):\n",
        "  tag = str(tag)\n",
        "  jnglr = str(jnglr)\n",
        "  arr = np.array(res)\n",
        "  bool_arr = np.array([(tag + \" \" + jnglr) in res[i][3] and len(res[i][3]) < 3 for i in range(len(res))])\n",
        "  if len(bool_arr) == 0:\n",
        "    ganks_wtime.append(np.array([0]*MAX_TIME_STEP))\n",
        "    continue\n",
        "  if len(arr[bool_arr]) == 0:\n",
        "    ganks_wtime.append(np.array([0]*MAX_TIME_STEP))\n",
        "    continue\n",
        "  ganks_wtime.append(arr[bool_arr])\n",
        "\n",
        "times = []\n",
        "for gank in ganks_wtime:\n",
        "  temp = []\n",
        "  if gank.shape[0] == 30:\n",
        "    temp.append([0])\n",
        "    times.append(temp)\n",
        "    continue\n",
        "  for k in range(gank.shape[0]):\n",
        "    temp.append(gank[k][0])\n",
        "  times.append(temp)\n",
        "\n",
        "bgank_counts = []\n",
        "for t in times:\n",
        "  mins = [int(np.floor(a)) for a in t]\n",
        "  counts = {a:mins.count(a) for a in range(1, MAX_TIME_STEP+1)}\n",
        "  total = 0\n",
        "  new = [0]*MAX_TIME_STEP\n",
        "  for i in range(MAX_TIME_STEP):\n",
        "    total += counts[i+1]\n",
        "    new[i] = total\n",
        "  bgank_counts.append(new)"
      ],
      "metadata": {
        "id": "AB2u-YMstDgS",
        "outputId": "fff00fe1-2519-4f79-e763-1972a659db8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AB2u-YMstDgS",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rkls = df_raw[\"rKills\"]\n",
        "rtags = df_raw[\"redTeamTag\"]\n",
        "rjng = df_raw[\"redJungle\"]\n",
        "ganks_wtime = []\n",
        "for jnglr, res, tag in zip(rjng, rkls, rtags):\n",
        "  tag = str(tag)\n",
        "  jnglr = str(jnglr)\n",
        "  arr = np.array(res)\n",
        "  bool_arr = np.array([(tag + \" \" + jnglr) in res[i][3] and len(res[i][3]) < 3 for i in range(len(res))])\n",
        "  if len(bool_arr) == 0:\n",
        "    ganks_wtime.append(np.array([0]*MAX_TIME_STEP))\n",
        "    continue\n",
        "  if len(arr[bool_arr]) == 0:\n",
        "    ganks_wtime.append(np.array([0]*MAX_TIME_STEP))\n",
        "    continue\n",
        "  ganks_wtime.append(arr[bool_arr])\n",
        "\n",
        "times = []\n",
        "for gank in ganks_wtime:\n",
        "  temp = []\n",
        "  if gank.shape[0] == 30:\n",
        "    temp.append([0])\n",
        "    times.append(temp)\n",
        "    continue\n",
        "  for k in range(gank.shape[0]):\n",
        "    temp.append(gank[k][0])\n",
        "  times.append(temp)\n",
        "\n",
        "rgank_counts = []\n",
        "for t in times:\n",
        "  mins = [int(np.floor(a)) for a in t]\n",
        "  counts = {a:mins.count(a) for a in range(1, MAX_TIME_STEP+1)}\n",
        "  total = 0\n",
        "  new = [0]*MAX_TIME_STEP\n",
        "  for i in range(MAX_TIME_STEP):\n",
        "    total += counts[i+1]\n",
        "    new[i] = total\n",
        "  rgank_counts.append(new)"
      ],
      "metadata": {
        "id": "IaNZbjd0uB5m",
        "outputId": "b07a4b2e-3f03-4ff2-f945-c107b949f18d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IaNZbjd0uB5m",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "bd378386-a4a8-431a-b243-2c1130397ec9",
      "metadata": {
        "id": "bd378386-a4a8-431a-b243-2c1130397ec9"
      },
      "outputs": [],
      "source": [
        "df[\"bGankTime\"] = bgank_counts\n",
        "df[\"rGankTime\"] = rgank_counts\n",
        "df[\"gankDiffs\"] = df[\"bGankTime\"].apply(np.array) - df[\"rGankTime\"].apply(np.array)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"gankDiffs\"]"
      ],
      "metadata": {
        "id": "MjcheqRl2t_L",
        "outputId": "fbfe3cbd-0670-48ea-aefc-4db9f2f99ea2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MjcheqRl2t_L",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, ...\n",
              "2       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, ...\n",
              "3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...\n",
              "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, ...\n",
              "                              ...                        \n",
              "6379    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1,...\n",
              "6380    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "6381    [0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, ...\n",
              "6382    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -...\n",
              "6383    [0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0,...\n",
              "Name: gankDiffs, Length: 6384, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "7323dcf0-4ff5-412b-9fcd-30bb3d6ec6f0",
      "metadata": {
        "id": "7323dcf0-4ff5-412b-9fcd-30bb3d6ec6f0"
      },
      "outputs": [],
      "source": [
        "stats = ['golddiff','dragondiff', 'barondiff', 'heralddiff', 'towerdiff', 'inhibitordiff', 'killdiff']\n",
        "x = df[stats]\n",
        "y = df['bResult']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "6876ac82-9e51-4cc2-96c4-14e87d1b6f54",
      "metadata": {
        "id": "6876ac82-9e51-4cc2-96c4-14e87d1b6f54",
        "outputId": "2bf2a7cc-3f90-47de-9253-d5d60d75e4e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of features per timestep: 7\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = {}\n",
        "scalers = {}\n",
        "for stat in stats:\n",
        "    scalers[stat] = StandardScaler()\n",
        "    for row in df[stat]:\n",
        "        scalers[stat].partial_fit(np.asanyarray(row).reshape(-1, 1))\n",
        "    data[stat] = [scalers[stat].transform(np.asanyarray(row).reshape(-1, 1)).reshape(-1) for row in df[stat]]\n",
        "\n",
        "num_features = len(data)\n",
        "print(f'# of features per timestep: {num_features}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "fe30b108-dcbe-4eea-94d5-02f754aea0e4",
      "metadata": {
        "id": "fe30b108-dcbe-4eea-94d5-02f754aea0e4"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, Dense, LSTM\n",
        "from sklearn.model_selection import TimeSeriesSplit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "RANDOM_SEED = 0\n",
        "\n",
        "class LOLDataset(Dataset):\n",
        "    def __init__(self, data, stats, label):\n",
        "        \n",
        "        self.data =[]\n",
        "        for t in range(MAX_TIME_STEP):\n",
        "            self.data.append([[data[stat][i][t] for stat in stats] for i in range(matches)])\n",
        "        self.label=[i for i in label]\n",
        "        \n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor([ [torch.scalar_tensor(i) for i in x[item]] for x in self.data]), torch.tensor(self.label[item])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "            "
      ],
      "metadata": {
        "id": "5FvgtxLO5i3S"
      },
      "id": "5FvgtxLO5i3S",
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNN,self).__init__()\n",
        "        self.hidden_size = 256\n",
        "        \n",
        "        self.rnn= nn.RNN(\n",
        "            nonlinearity = 'relu',\n",
        "            input_size = num_features,\n",
        "            hidden_size = self.hidden_size,\n",
        "            num_layers = 1,\n",
        "            batch_first = True\n",
        "        )\n",
        "\n",
        "        self.out = nn.Linear(self.hidden_size, 2)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        r_out, hn = self.rnn(x, torch.zeros(1, len(x), self.hidden_size))\n",
        "        out = self.out(r_out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "paT6Ey0D7iI6"
      },
      "id": "paT6Ey0D7iI6",
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "dataset = LOLDataset(data, stats, df[\"bResult\"])\n",
        "test_size = valid_size = int(0.2 * len(dataset))\n",
        "train_size = len(dataset) - test_size - valid_size\n",
        "\n",
        "trainDataset, validDataset, testDataset = random_split(\n",
        "    dataset = dataset,\n",
        "    lengths = [train_size, valid_size, test_size],\n",
        "    generator = torch.Generator().manual_seed(0)\n",
        ")\n",
        "\n",
        "trainLoader = DataLoader(trainDataset, batch_size = BATCH_SIZE, shuffle=True)\n",
        "validLoader = DataLoader(validDataset, batch_size = BATCH_SIZE)\n",
        "testLoader = DataLoader(testDataset, batch_size = BATCH_SIZE)"
      ],
      "metadata": {
        "id": "UrIElHAX7n5Y"
      },
      "id": "UrIElHAX7n5Y",
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "for x, y in dataset:\n",
        "    if (x[-1][0] > 0) ^ (y == 1) == 0 : correct += 1\n",
        "print(f'Baseline Accuracy = {correct/matches*100:>.2f}% ')"
      ],
      "metadata": {
        "id": "ok51Z1jN77uZ",
        "outputId": "c0fdd58c-4e5b-45bd-eaa4-579507744e5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ok51Z1jN77uZ",
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy = 83.24% \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer, mute = False):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = Variable(x), Variable(y)\n",
        "\n",
        "        predict = model(x)\n",
        "        loss = loss_fn(predict, y)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 30 == 0 and not mute:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "Lv9Tnkdf8BTs"
      },
      "id": "Lv9Tnkdf8BTs",
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn, validation = False):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    \n",
        "    correct = 0\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for step,(x,y) in enumerate(dataloader):\n",
        "            x, y = Variable(x), Variable(y)\n",
        "            predict = model(x)\n",
        "            test_loss += loss_fn(predict, y).item()\n",
        "            correct += (predict.argmax(1) == y).sum().item()\n",
        "    \n",
        "    print(f\"{'Valid' if validation else 'Test'} Acc:{correct/size:>7f}, Avg Loss: {test_loss/size:>7f}\")\n",
        "    \n",
        "    return correct/size"
      ],
      "metadata": {
        "id": "ErYMOGIZ8Feo"
      },
      "id": "ErYMOGIZ8Feo",
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MUTE = False\n",
        "EPOCH = 100\n",
        "LR = 0.0001\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "model = RNN()\n",
        "print(model)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "best_acc = 0\n",
        "early_stopping = 0\n",
        "early_stopping_threshold = 5\n",
        "\n",
        "for epoch in range(1, EPOCH + 1):\n",
        "    print(f\"--------- Epoch #{epoch} ---------\")\n",
        "    train(trainLoader, model, loss_func, optimizer, mute = MUTE)\n",
        "    valid_acc = test(validLoader, model, loss_func, validation = True)\n",
        "    if valid_acc > best_acc :\n",
        "        early_stopping = 0\n",
        "        best_acc = valid_acc\n",
        "        torch.save(model.state_dict(), f\"./{MAX_TIME_STEP}.pt\")\n",
        "    else :\n",
        "        early_stopping += 1\n",
        "        if early_stopping == early_stopping_threshold :\n",
        "            print(f\"Early stopped at epoch #{epoch} with best validation accuracy {best_acc*100:.2f}%.\")\n",
        "            break"
      ],
      "metadata": {
        "id": "21PlFpcz8J3Y",
        "outputId": "400af2f4-152e-4196-87b2-0eb7deafa7e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "21PlFpcz8J3Y",
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (rnn): RNN(7, 256, batch_first=True)\n",
            "  (out): Linear(in_features=256, out_features=2, bias=True)\n",
            ")\n",
            "--------- Epoch #1 ---------\n",
            "loss: 0.714780  [    0/ 3832]\n",
            "loss: 0.501933  [  960/ 3832]\n",
            "loss: 0.389922  [ 1920/ 3832]\n",
            "loss: 0.324179  [ 2880/ 3832]\n",
            "Valid Acc:0.835423, Avg Loss: 0.011742\n",
            "--------- Epoch #2 ---------\n",
            "loss: 0.506647  [    0/ 3832]\n",
            "loss: 0.382695  [  960/ 3832]\n",
            "loss: 0.282866  [ 1920/ 3832]\n",
            "loss: 0.313975  [ 2880/ 3832]\n",
            "Valid Acc:0.830721, Avg Loss: 0.011453\n",
            "--------- Epoch #3 ---------\n",
            "loss: 0.223891  [    0/ 3832]\n",
            "loss: 0.422983  [  960/ 3832]\n",
            "loss: 0.356945  [ 1920/ 3832]\n",
            "loss: 0.322592  [ 2880/ 3832]\n",
            "Valid Acc:0.833072, Avg Loss: 0.011359\n",
            "--------- Epoch #4 ---------\n",
            "loss: 0.215209  [    0/ 3832]\n",
            "loss: 0.255946  [  960/ 3832]\n",
            "loss: 0.361895  [ 1920/ 3832]\n",
            "loss: 0.356493  [ 2880/ 3832]\n",
            "Valid Acc:0.833072, Avg Loss: 0.011274\n",
            "--------- Epoch #5 ---------\n",
            "loss: 0.268962  [    0/ 3832]\n",
            "loss: 0.366829  [  960/ 3832]\n",
            "loss: 0.224978  [ 1920/ 3832]\n",
            "loss: 0.198061  [ 2880/ 3832]\n",
            "Valid Acc:0.837774, Avg Loss: 0.011223\n",
            "--------- Epoch #6 ---------\n",
            "loss: 0.363072  [    0/ 3832]\n",
            "loss: 0.265811  [  960/ 3832]\n",
            "loss: 0.193504  [ 1920/ 3832]\n",
            "loss: 0.378817  [ 2880/ 3832]\n",
            "Valid Acc:0.833072, Avg Loss: 0.011185\n",
            "--------- Epoch #7 ---------\n",
            "loss: 0.425379  [    0/ 3832]\n",
            "loss: 0.475442  [  960/ 3832]\n",
            "loss: 0.340355  [ 1920/ 3832]\n",
            "loss: 0.340923  [ 2880/ 3832]\n",
            "Valid Acc:0.830721, Avg Loss: 0.011187\n",
            "--------- Epoch #8 ---------\n",
            "loss: 0.348933  [    0/ 3832]\n",
            "loss: 0.165611  [  960/ 3832]\n",
            "loss: 0.294496  [ 1920/ 3832]\n",
            "loss: 0.399523  [ 2880/ 3832]\n",
            "Valid Acc:0.839342, Avg Loss: 0.011173\n",
            "--------- Epoch #9 ---------\n",
            "loss: 0.320485  [    0/ 3832]\n",
            "loss: 0.311250  [  960/ 3832]\n",
            "loss: 0.305696  [ 1920/ 3832]\n",
            "loss: 0.331177  [ 2880/ 3832]\n",
            "Valid Acc:0.834639, Avg Loss: 0.011139\n",
            "--------- Epoch #10 ---------\n",
            "loss: 0.402051  [    0/ 3832]\n",
            "loss: 0.501506  [  960/ 3832]\n",
            "loss: 0.283349  [ 1920/ 3832]\n",
            "loss: 0.367094  [ 2880/ 3832]\n",
            "Valid Acc:0.833072, Avg Loss: 0.011204\n",
            "--------- Epoch #11 ---------\n",
            "loss: 0.321475  [    0/ 3832]\n",
            "loss: 0.321909  [  960/ 3832]\n",
            "loss: 0.191615  [ 1920/ 3832]\n",
            "loss: 0.514767  [ 2880/ 3832]\n",
            "Valid Acc:0.829937, Avg Loss: 0.011633\n",
            "--------- Epoch #12 ---------\n",
            "loss: 0.447590  [    0/ 3832]\n",
            "loss: 0.242931  [  960/ 3832]\n",
            "loss: 0.271426  [ 1920/ 3832]\n",
            "loss: 0.260589  [ 2880/ 3832]\n",
            "Valid Acc:0.836991, Avg Loss: 0.011315\n",
            "--------- Epoch #13 ---------\n",
            "loss: 0.518545  [    0/ 3832]\n",
            "loss: 0.253810  [  960/ 3832]\n",
            "loss: 0.241969  [ 1920/ 3832]\n",
            "loss: 0.463438  [ 2880/ 3832]\n",
            "Valid Acc:0.834639, Avg Loss: 0.011346\n",
            "Early stopped at epoch #13 with best validation accuracy 83.93%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(f\"./{MAX_TIME_STEP}.pt\"))\n",
        "acc_RNN = test(testLoader, model, loss_func)\n",
        "\n",
        "print(f'Model Accuracy = {acc_RNN*100:>.2f}% ')\n"
      ],
      "metadata": {
        "id": "4FRrmfO78eCg",
        "outputId": "c88474da-0259-400c-8ea9-4edef9b40908",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4FRrmfO78eCg",
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Acc:0.848746, Avg Loss: 0.010259\n",
            "Model Accuracy = 84.87% \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}