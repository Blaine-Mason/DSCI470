{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52521e43-7770-4068-9f69-70901926070b",
   "metadata": {
    "id": "52521e43-7770-4068-9f69-70901926070b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "MAX_TIME_STEP = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "V4GHHYFc1TkS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V4GHHYFc1TkS",
    "outputId": "6c761022-3848-49e4-c18f-54da3c0fd141"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "132d2036-fab5-4318-9102-94fb2020f4b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "132d2036-fab5-4318-9102-94fb2020f4b1",
    "outputId": "64407acd-e1af-4108-ad79-11c4ecc3251e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6384\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(\"/content/drive/MyDrive/data/archive(1)/LeagueofLegends.csv\")\n",
    "df_raw = df_raw[df_raw['gamelength'] >= MAX_TIME_STEP]\n",
    "df_raw.reset_index(drop = True, inplace = True)\n",
    "print(len(df_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1ad73e9-8f09-42f9-960d-680d12fd6055",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1ad73e9-8f09-42f9-960d-680d12fd6055",
    "outputId": "0c80ecac-324d-4f89-f192-d203ec1d7ae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6384\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/content/drive/MyDrive/data/archive(1)/LeagueofLegends.csv\")\n",
    "df = df[df['gamelength'] >= MAX_TIME_STEP]\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "matches = len(df)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3d76baf-f578-4708-8f90-3afe59ae9ba2",
   "metadata": {
    "id": "c3d76baf-f578-4708-8f90-3afe59ae9ba2"
   },
   "outputs": [],
   "source": [
    "def count_item(items):\n",
    "    count = np.zeros(MAX_TIME_STEP, dtype=np.int8)\n",
    "    for timestep in range(MAX_TIME_STEP) :\n",
    "        for item in items:\n",
    "            if item[0] <= timestep + 1:\n",
    "                count[timestep] += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7fa85187-eb0d-4f84-a5f5-4844d2609f81",
   "metadata": {
    "id": "7fa85187-eb0d-4f84-a5f5-4844d2609f81"
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "df['golddiff'] = df['golddiff'].apply(literal_eval)\n",
    "blue = ['bDragons', 'bBarons', 'bHeralds', 'bTowers', 'bInhibs', 'bKills']\n",
    "red = ['rDragons', 'rBarons', 'rHeralds', 'rTowers', 'rInhibs', 'rKills']\n",
    "diffs = ['dragondiff', 'barondiff', 'heralddiff', 'towerdiff', 'inhibitordiff', 'killdiff']\n",
    "for r, b, d in zip(blue, red, diffs):\n",
    "    df[b] = df[b].apply(literal_eval)\n",
    "    df[r] = df[r].apply(literal_eval)\n",
    "\n",
    "    df[b] = df[b].apply(count_item)\n",
    "    df[r] = df[r].apply(count_item)\n",
    "    df[d] = df[b] - df[r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8f5e620-6f7f-462a-8dcf-05cd705a6b6a",
   "metadata": {
    "id": "c8f5e620-6f7f-462a-8dcf-05cd705a6b6a"
   },
   "outputs": [],
   "source": [
    "df_raw[\"bKills\"] = df_raw[\"bKills\"].apply(literal_eval)\n",
    "df_raw[\"rKills\"] = df_raw[\"rKills\"].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "AB2u-YMstDgS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AB2u-YMstDgS",
    "outputId": "fff00fe1-2519-4f79-e763-1972a659db8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "bkls = df_raw[\"bKills\"]\n",
    "btags = df_raw[\"blueTeamTag\"]\n",
    "bjng = df_raw[\"blueJungle\"]\n",
    "ganks_wtime = []\n",
    "for jnglr, res, tag in zip(bjng, bkls, btags):\n",
    "  tag = str(tag)\n",
    "  jnglr = str(jnglr)\n",
    "  arr = np.array(res)\n",
    "  bool_arr = np.array([(tag + \" \" + jnglr) in res[i][3] and len(res[i][3]) < 3 for i in range(len(res))])\n",
    "  if len(bool_arr) == 0:\n",
    "    ganks_wtime.append(np.array([0]*MAX_TIME_STEP))\n",
    "    continue\n",
    "  if len(arr[bool_arr]) == 0:\n",
    "    ganks_wtime.append(np.array([0]*MAX_TIME_STEP))\n",
    "    continue\n",
    "  ganks_wtime.append(arr[bool_arr])\n",
    "\n",
    "times = []\n",
    "for gank in ganks_wtime:\n",
    "  temp = []\n",
    "  if gank.shape[0] == 30:\n",
    "    temp.append([0])\n",
    "    times.append(temp)\n",
    "    continue\n",
    "  for k in range(gank.shape[0]):\n",
    "    temp.append(gank[k][0])\n",
    "  times.append(temp)\n",
    "\n",
    "bgank_counts = []\n",
    "for t in times:\n",
    "  mins = [int(np.floor(a)) for a in t]\n",
    "  counts = {a:mins.count(a) for a in range(1, MAX_TIME_STEP+1)}\n",
    "  total = 0\n",
    "  new = [0]*MAX_TIME_STEP\n",
    "  for i in range(MAX_TIME_STEP):\n",
    "    total += counts[i+1]\n",
    "    new[i] = total\n",
    "  bgank_counts.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "IaNZbjd0uB5m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IaNZbjd0uB5m",
    "outputId": "b07a4b2e-3f03-4ff2-f945-c107b949f18d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "rkls = df_raw[\"rKills\"]\n",
    "rtags = df_raw[\"redTeamTag\"]\n",
    "rjng = df_raw[\"redJungle\"]\n",
    "ganks_wtime = []\n",
    "for jnglr, res, tag in zip(rjng, rkls, rtags):\n",
    "  tag = str(tag)\n",
    "  jnglr = str(jnglr)\n",
    "  arr = np.array(res)\n",
    "  bool_arr = np.array([(tag + \" \" + jnglr) in res[i][3] and len(res[i][3]) < 3 for i in range(len(res))])\n",
    "  if len(bool_arr) == 0:\n",
    "    ganks_wtime.append(np.array([0]*MAX_TIME_STEP))\n",
    "    continue\n",
    "  if len(arr[bool_arr]) == 0:\n",
    "    ganks_wtime.append(np.array([0]*MAX_TIME_STEP))\n",
    "    continue\n",
    "  ganks_wtime.append(arr[bool_arr])\n",
    "\n",
    "times = []\n",
    "for gank in ganks_wtime:\n",
    "  temp = []\n",
    "  if gank.shape[0] == 30:\n",
    "    temp.append([0])\n",
    "    times.append(temp)\n",
    "    continue\n",
    "  for k in range(gank.shape[0]):\n",
    "    temp.append(gank[k][0])\n",
    "  times.append(temp)\n",
    "\n",
    "rgank_counts = []\n",
    "for t in times:\n",
    "  mins = [int(np.floor(a)) for a in t]\n",
    "  counts = {a:mins.count(a) for a in range(1, MAX_TIME_STEP+1)}\n",
    "  total = 0\n",
    "  new = [0]*MAX_TIME_STEP\n",
    "  for i in range(MAX_TIME_STEP):\n",
    "    total += counts[i+1]\n",
    "    new[i] = total\n",
    "  rgank_counts.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bd378386-a4a8-431a-b243-2c1130397ec9",
   "metadata": {
    "id": "bd378386-a4a8-431a-b243-2c1130397ec9"
   },
   "outputs": [],
   "source": [
    "df[\"bGankTime\"] = bgank_counts\n",
    "df[\"rGankTime\"] = rgank_counts\n",
    "df[\"gankDiffs\"] = df[\"bGankTime\"].apply(np.array) - df[\"rGankTime\"].apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0tu_83f2ETym",
   "metadata": {
    "id": "0tu_83f2ETym"
   },
   "outputs": [],
   "source": [
    "stats = ['golddiff','dragondiff', 'barondiff', 'heralddiff', 'towerdiff', 'inhibitordiff', 'killdiff']\n",
    "x = df[stats]\n",
    "y = df['bResult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "JFu7iuwIFP84",
   "metadata": {
    "id": "JFu7iuwIFP84"
   },
   "outputs": [],
   "source": [
    "arrs = x[\"golddiff\"].apply(np.array)\n",
    "first_30 = []\n",
    "for a in arrs:\n",
    "  first_30.append(a[0:30])\n",
    "\n",
    "sum_ = [0]*MAX_TIME_STEP\n",
    "for l in first_30:\n",
    "  sum_ += l\n",
    "sum_ = sum_/len(first_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fe30b108-dcbe-4eea-94d5-02f754aea0e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe30b108-dcbe-4eea-94d5-02f754aea0e4",
    "outputId": "81223021-3b26-43c5-fe6a-275e2c927c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of features per timestep: 7\n"
     ]
    }
   ],
   "source": [
    "stats = ['golddiff','dragondiff', 'barondiff', 'heralddiff', 'towerdiff', 'inhibitordiff', 'killdiff']\n",
    "x = df[stats]\n",
    "y = df['bResult']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = {}\n",
    "scalers = {}\n",
    "for stat in stats:\n",
    "    scalers[stat] = StandardScaler()\n",
    "    for row in df[stat]:\n",
    "        scalers[stat].partial_fit(np.asanyarray(row).reshape(-1, 1))\n",
    "    data[stat] = [scalers[stat].transform(np.asanyarray(row).reshape(-1, 1)).reshape(-1) for row in df[stat]]\n",
    "\n",
    "num_features = len(data)\n",
    "print(f'# of features per timestep: {num_features}')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, LSTM\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "class LOLDataset(Dataset):\n",
    "    def __init__(self, data, stats, label):\n",
    "        \n",
    "        self.data =[]\n",
    "        for t in range(MAX_TIME_STEP):\n",
    "            self.data.append([[data[stat][i][t] for stat in stats] for i in range(matches)])\n",
    "        self.label=[i for i in label]\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor([ [torch.scalar_tensor(i) for i in x[item]] for x in self.data]), torch.tensor(self.label[item])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN,self).__init__()\n",
    "        self.hidden_size = 256\n",
    "        \n",
    "        self.rnn= nn.RNN(\n",
    "            nonlinearity = 'relu',\n",
    "            input_size = num_features,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = 1,\n",
    "            batch_first = True\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(self.hidden_size, 2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        r_out, hn = self.rnn(x, torch.zeros(1, len(x), self.hidden_size))\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, mute = False):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = Variable(x), Variable(y)\n",
    "\n",
    "        predict = model(x)\n",
    "        loss = loss_fn(predict, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 30 == 0 and not mute:\n",
    "            loss, current = loss.item(), batch * len(x)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn, validation = False):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    correct = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step,(x,y) in enumerate(dataloader):\n",
    "            x, y = Variable(x), Variable(y)\n",
    "            predict = model(x)\n",
    "            test_loss += loss_fn(predict, y).item()\n",
    "            correct += (predict.argmax(1) == y).sum().item()\n",
    "    \n",
    "    print(f\"{'Valid' if validation else 'Test'} Acc:{correct/size:>7f}, Avg Loss: {test_loss/size:>7f}\")\n",
    "    \n",
    "    return correct/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "UrIElHAX7n5Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UrIElHAX7n5Y",
    "outputId": "9fe3eadd-d748-4575-a90c-8093ee195fbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy = 83.24% \n",
      "RNN(\n",
      "  (rnn): RNN(7, 256, batch_first=True)\n",
      "  (out): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n",
      "--------- Epoch #1 ---------\n",
      "loss: 0.714780  [    0/ 3832]\n",
      "loss: 0.501933  [  960/ 3832]\n",
      "loss: 0.389922  [ 1920/ 3832]\n",
      "loss: 0.324179  [ 2880/ 3832]\n",
      "Valid Acc:0.835423, Avg Loss: 0.011742\n",
      "--------- Epoch #2 ---------\n",
      "loss: 0.506647  [    0/ 3832]\n",
      "loss: 0.382695  [  960/ 3832]\n",
      "loss: 0.282866  [ 1920/ 3832]\n",
      "loss: 0.313975  [ 2880/ 3832]\n",
      "Valid Acc:0.830721, Avg Loss: 0.011453\n",
      "--------- Epoch #3 ---------\n",
      "loss: 0.223891  [    0/ 3832]\n",
      "loss: 0.422983  [  960/ 3832]\n",
      "loss: 0.356945  [ 1920/ 3832]\n",
      "loss: 0.322592  [ 2880/ 3832]\n",
      "Valid Acc:0.833072, Avg Loss: 0.011359\n",
      "--------- Epoch #4 ---------\n",
      "loss: 0.215209  [    0/ 3832]\n",
      "loss: 0.255946  [  960/ 3832]\n",
      "loss: 0.361895  [ 1920/ 3832]\n",
      "loss: 0.356493  [ 2880/ 3832]\n",
      "Valid Acc:0.833072, Avg Loss: 0.011274\n",
      "--------- Epoch #5 ---------\n",
      "loss: 0.268962  [    0/ 3832]\n",
      "loss: 0.366829  [  960/ 3832]\n",
      "loss: 0.224978  [ 1920/ 3832]\n",
      "loss: 0.198061  [ 2880/ 3832]\n",
      "Valid Acc:0.837774, Avg Loss: 0.011223\n",
      "--------- Epoch #6 ---------\n",
      "loss: 0.363072  [    0/ 3832]\n",
      "loss: 0.265811  [  960/ 3832]\n",
      "loss: 0.193504  [ 1920/ 3832]\n",
      "loss: 0.378817  [ 2880/ 3832]\n",
      "Valid Acc:0.833072, Avg Loss: 0.011185\n",
      "--------- Epoch #7 ---------\n",
      "loss: 0.425379  [    0/ 3832]\n",
      "loss: 0.475442  [  960/ 3832]\n",
      "loss: 0.340355  [ 1920/ 3832]\n",
      "loss: 0.340923  [ 2880/ 3832]\n",
      "Valid Acc:0.830721, Avg Loss: 0.011187\n",
      "--------- Epoch #8 ---------\n",
      "loss: 0.348933  [    0/ 3832]\n",
      "loss: 0.165611  [  960/ 3832]\n",
      "loss: 0.294496  [ 1920/ 3832]\n",
      "loss: 0.399523  [ 2880/ 3832]\n",
      "Valid Acc:0.839342, Avg Loss: 0.011173\n",
      "--------- Epoch #9 ---------\n",
      "loss: 0.320485  [    0/ 3832]\n",
      "loss: 0.311250  [  960/ 3832]\n",
      "loss: 0.305696  [ 1920/ 3832]\n",
      "loss: 0.331177  [ 2880/ 3832]\n",
      "Valid Acc:0.834639, Avg Loss: 0.011139\n",
      "--------- Epoch #10 ---------\n",
      "loss: 0.402051  [    0/ 3832]\n",
      "loss: 0.501506  [  960/ 3832]\n",
      "loss: 0.283349  [ 1920/ 3832]\n",
      "loss: 0.367094  [ 2880/ 3832]\n",
      "Valid Acc:0.833072, Avg Loss: 0.011204\n",
      "--------- Epoch #11 ---------\n",
      "loss: 0.321475  [    0/ 3832]\n",
      "loss: 0.321909  [  960/ 3832]\n",
      "loss: 0.191615  [ 1920/ 3832]\n",
      "loss: 0.514767  [ 2880/ 3832]\n",
      "Valid Acc:0.829937, Avg Loss: 0.011633\n",
      "--------- Epoch #12 ---------\n",
      "loss: 0.447590  [    0/ 3832]\n",
      "loss: 0.242931  [  960/ 3832]\n",
      "loss: 0.271426  [ 1920/ 3832]\n",
      "loss: 0.260589  [ 2880/ 3832]\n",
      "Valid Acc:0.836991, Avg Loss: 0.011315\n",
      "--------- Epoch #13 ---------\n",
      "loss: 0.518545  [    0/ 3832]\n",
      "loss: 0.253810  [  960/ 3832]\n",
      "loss: 0.241969  [ 1920/ 3832]\n",
      "loss: 0.463438  [ 2880/ 3832]\n",
      "Valid Acc:0.834639, Avg Loss: 0.011346\n",
      "Early stopped at epoch #13 with best validation accuracy 83.93%.\n",
      "Test Acc:0.848746, Avg Loss: 0.010259\n",
      "Model Accuracy = 84.87% \n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "dataset = LOLDataset(data, stats, df[\"bResult\"])\n",
    "test_size = valid_size = int(0.2 * len(dataset))\n",
    "train_size = len(dataset) - test_size - valid_size\n",
    "\n",
    "trainDataset, validDataset, testDataset = random_split(\n",
    "    dataset = dataset,\n",
    "    lengths = [train_size, valid_size, test_size],\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    ")\n",
    "\n",
    "trainLoader = DataLoader(trainDataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "validLoader = DataLoader(validDataset, batch_size = BATCH_SIZE)\n",
    "testLoader = DataLoader(testDataset, batch_size = BATCH_SIZE)\n",
    "\n",
    "correct = 0\n",
    "for x, y in dataset:\n",
    "    if (x[-1][0] > 0) ^ (y == 1) == 0 : correct += 1\n",
    "print(f'Baseline Accuracy = {correct/matches*100:>.2f}% ')\n",
    "\n",
    "MUTE = False\n",
    "EPOCH = 100\n",
    "LR = 0.0001\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "model = RNN()\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "best_acc = 0\n",
    "early_stopping = 0\n",
    "early_stopping_threshold = 5\n",
    "\n",
    "for epoch in range(1, EPOCH + 1):\n",
    "    print(f\"--------- Epoch #{epoch} ---------\")\n",
    "    train(trainLoader, model, loss_func, optimizer, mute = MUTE)\n",
    "    valid_acc = test(validLoader, model, loss_func, validation = True)\n",
    "    if valid_acc > best_acc :\n",
    "        early_stopping = 0\n",
    "        best_acc = valid_acc\n",
    "        torch.save(model.state_dict(), f\"./{MAX_TIME_STEP}.pt\")\n",
    "    else :\n",
    "        early_stopping += 1\n",
    "        if early_stopping == early_stopping_threshold :\n",
    "            print(f\"Early stopped at epoch #{epoch} with best validation accuracy {best_acc*100:.2f}%.\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load(f\"./{MAX_TIME_STEP}.pt\"))\n",
    "acc_RNN = test(testLoader, model, loss_func)\n",
    "\n",
    "print(f'Model Accuracy = {acc_RNN*100:>.2f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "SGm-ebQNAFhF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SGm-ebQNAFhF",
    "outputId": "767de673-ddea-41de-a227-924a8994ac22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of features per timestep: 8\n"
     ]
    }
   ],
   "source": [
    "stats = ['golddiff','dragondiff', 'barondiff', 'heralddiff', 'towerdiff', 'inhibitordiff', 'killdiff', 'gankDiffs']\n",
    "x = df[stats]\n",
    "y = df['bResult']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = {}\n",
    "scalers = {}\n",
    "for stat in stats:\n",
    "    scalers[stat] = StandardScaler()\n",
    "    for row in df[stat]:\n",
    "        scalers[stat].partial_fit(np.asanyarray(row).reshape(-1, 1))\n",
    "    data[stat] = [scalers[stat].transform(np.asanyarray(row).reshape(-1, 1)).reshape(-1) for row in df[stat]]\n",
    "\n",
    "num_features = len(data)\n",
    "print(f'# of features per timestep: {num_features}')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, LSTM\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "class LOLDataset(Dataset):\n",
    "    def __init__(self, data, stats, label):\n",
    "        \n",
    "        self.data =[]\n",
    "        for t in range(MAX_TIME_STEP):\n",
    "            self.data.append([[data[stat][i][t] for stat in stats] for i in range(matches)])\n",
    "        self.label=[i for i in label]\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor([ [torch.scalar_tensor(i) for i in x[item]] for x in self.data]), torch.tensor(self.label[item])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN,self).__init__()\n",
    "        self.hidden_size = 256\n",
    "        \n",
    "        self.rnn= nn.RNN(\n",
    "            nonlinearity = 'relu',\n",
    "            input_size = num_features,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = 1,\n",
    "            batch_first = True\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(self.hidden_size, 2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        r_out, hn = self.rnn(x, torch.zeros(1, len(x), self.hidden_size))\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, mute = False):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = Variable(x), Variable(y)\n",
    "\n",
    "        predict = model(x)\n",
    "        loss = loss_fn(predict, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 30 == 0 and not mute:\n",
    "            loss, current = loss.item(), batch * len(x)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn, validation = False):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    correct = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step,(x,y) in enumerate(dataloader):\n",
    "            x, y = Variable(x), Variable(y)\n",
    "            predict = model(x)\n",
    "            test_loss += loss_fn(predict, y).item()\n",
    "            correct += (predict.argmax(1) == y).sum().item()\n",
    "    \n",
    "    print(f\"{'Valid' if validation else 'Test'} Acc:{correct/size:>7f}, Avg Loss: {test_loss/size:>7f}\")\n",
    "    \n",
    "    return correct/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "wgVl5vekATrY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wgVl5vekATrY",
    "outputId": "aeffd9f2-bd19-49b0-ae5d-9700536b147d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy = 83.24% \n",
      "RNN(\n",
      "  (rnn): RNN(8, 256, batch_first=True)\n",
      "  (out): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n",
      "--------- Epoch #1 ---------\n",
      "loss: 0.693780  [    0/ 3832]\n",
      "loss: 0.530695  [  960/ 3832]\n",
      "loss: 0.593355  [ 1920/ 3832]\n",
      "loss: 0.349287  [ 2880/ 3832]\n",
      "Valid Acc:0.829937, Avg Loss: 0.011958\n",
      "--------- Epoch #2 ---------\n",
      "loss: 0.471251  [    0/ 3832]\n",
      "loss: 0.274079  [  960/ 3832]\n",
      "loss: 0.421325  [ 1920/ 3832]\n",
      "loss: 0.387750  [ 2880/ 3832]\n",
      "Valid Acc:0.832288, Avg Loss: 0.011434\n",
      "--------- Epoch #3 ---------\n",
      "loss: 0.213679  [    0/ 3832]\n",
      "loss: 0.329957  [  960/ 3832]\n",
      "loss: 0.201297  [ 1920/ 3832]\n",
      "loss: 0.279747  [ 2880/ 3832]\n",
      "Valid Acc:0.830721, Avg Loss: 0.011414\n",
      "--------- Epoch #4 ---------\n",
      "loss: 0.427741  [    0/ 3832]\n",
      "loss: 0.457525  [  960/ 3832]\n",
      "loss: 0.443051  [ 1920/ 3832]\n",
      "loss: 0.211786  [ 2880/ 3832]\n",
      "Valid Acc:0.832288, Avg Loss: 0.011271\n",
      "--------- Epoch #5 ---------\n",
      "loss: 0.371048  [    0/ 3832]\n",
      "loss: 0.370235  [  960/ 3832]\n",
      "loss: 0.133813  [ 1920/ 3832]\n",
      "loss: 0.348875  [ 2880/ 3832]\n",
      "Valid Acc:0.836207, Avg Loss: 0.011243\n",
      "--------- Epoch #6 ---------\n",
      "loss: 0.317615  [    0/ 3832]\n",
      "loss: 0.145733  [  960/ 3832]\n",
      "loss: 0.690562  [ 1920/ 3832]\n",
      "loss: 0.222300  [ 2880/ 3832]\n",
      "Valid Acc:0.835423, Avg Loss: 0.011236\n",
      "--------- Epoch #7 ---------\n",
      "loss: 0.322452  [    0/ 3832]\n",
      "loss: 0.294484  [  960/ 3832]\n",
      "loss: 0.457641  [ 1920/ 3832]\n",
      "loss: 0.249013  [ 2880/ 3832]\n",
      "Valid Acc:0.836207, Avg Loss: 0.011472\n",
      "--------- Epoch #8 ---------\n",
      "loss: 0.425928  [    0/ 3832]\n",
      "loss: 0.214657  [  960/ 3832]\n",
      "loss: 0.374044  [ 1920/ 3832]\n",
      "loss: 0.202078  [ 2880/ 3832]\n",
      "Valid Acc:0.830721, Avg Loss: 0.011352\n",
      "--------- Epoch #9 ---------\n",
      "loss: 0.645960  [    0/ 3832]\n",
      "loss: 0.404018  [  960/ 3832]\n",
      "loss: 0.421118  [ 1920/ 3832]\n",
      "loss: 0.363785  [ 2880/ 3832]\n",
      "Valid Acc:0.840125, Avg Loss: 0.011215\n",
      "--------- Epoch #10 ---------\n",
      "loss: 0.534100  [    0/ 3832]\n",
      "loss: 0.322359  [  960/ 3832]\n",
      "loss: 0.237086  [ 1920/ 3832]\n",
      "loss: 0.286858  [ 2880/ 3832]\n",
      "Valid Acc:0.834639, Avg Loss: 0.011299\n",
      "--------- Epoch #11 ---------\n",
      "loss: 0.636522  [    0/ 3832]\n",
      "loss: 0.293651  [  960/ 3832]\n",
      "loss: 0.390151  [ 1920/ 3832]\n",
      "loss: 0.273454  [ 2880/ 3832]\n",
      "Valid Acc:0.832288, Avg Loss: 0.011371\n",
      "--------- Epoch #12 ---------\n",
      "loss: 0.254969  [    0/ 3832]\n",
      "loss: 0.304961  [  960/ 3832]\n",
      "loss: 0.397828  [ 1920/ 3832]\n",
      "loss: 0.337784  [ 2880/ 3832]\n",
      "Valid Acc:0.833856, Avg Loss: 0.011268\n",
      "--------- Epoch #13 ---------\n",
      "loss: 0.281016  [    0/ 3832]\n",
      "loss: 0.408773  [  960/ 3832]\n",
      "loss: 0.456087  [ 1920/ 3832]\n",
      "loss: 0.311859  [ 2880/ 3832]\n",
      "Valid Acc:0.833856, Avg Loss: 0.011354\n",
      "--------- Epoch #14 ---------\n",
      "loss: 0.318063  [    0/ 3832]\n",
      "loss: 0.300684  [  960/ 3832]\n",
      "loss: 0.277600  [ 1920/ 3832]\n",
      "loss: 0.298786  [ 2880/ 3832]\n",
      "Valid Acc:0.836207, Avg Loss: 0.011442\n",
      "Early stopped at epoch #14 with best validation accuracy 84.01%.\n",
      "Test Acc:0.847962, Avg Loss: 0.010112\n",
      "Model Accuracy = 84.80% \n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "dataset = LOLDataset(data, stats, df[\"bResult\"])\n",
    "test_size = valid_size = int(0.2 * len(dataset))\n",
    "train_size = len(dataset) - test_size - valid_size\n",
    "\n",
    "trainDataset, validDataset, testDataset = random_split(\n",
    "    dataset = dataset,\n",
    "    lengths = [train_size, valid_size, test_size],\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    ")\n",
    "\n",
    "trainLoader = DataLoader(trainDataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "validLoader = DataLoader(validDataset, batch_size = BATCH_SIZE)\n",
    "testLoader = DataLoader(testDataset, batch_size = BATCH_SIZE)\n",
    "\n",
    "correct = 0\n",
    "for x, y in dataset:\n",
    "    if (x[-1][0] > 0) ^ (y == 1) == 0 : correct += 1\n",
    "print(f'Baseline Accuracy = {correct/matches*100:>.2f}% ')\n",
    "\n",
    "MUTE = False\n",
    "EPOCH = 100\n",
    "LR = 0.0001\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "model = RNN()\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "best_acc = 0\n",
    "early_stopping = 0\n",
    "early_stopping_threshold = 5\n",
    "\n",
    "for epoch in range(1, EPOCH + 1):\n",
    "    print(f\"--------- Epoch #{epoch} ---------\")\n",
    "    train(trainLoader, model, loss_func, optimizer, mute = MUTE)\n",
    "    valid_acc = test(validLoader, model, loss_func, validation = True)\n",
    "    if valid_acc > best_acc :\n",
    "        early_stopping = 0\n",
    "        best_acc = valid_acc\n",
    "        torch.save(model.state_dict(), f\"./{MAX_TIME_STEP}.pt\")\n",
    "    else :\n",
    "        early_stopping += 1\n",
    "        if early_stopping == early_stopping_threshold :\n",
    "            print(f\"Early stopped at epoch #{epoch} with best validation accuracy {best_acc*100:.2f}%.\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load(f\"./{MAX_TIME_STEP}.pt\"))\n",
    "acc_RNN = test(testLoader, model, loss_func)\n",
    "\n",
    "print(f'Model Accuracy = {acc_RNN*100:>.2f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lgZnT9aWAXLi",
   "metadata": {
    "id": "lgZnT9aWAXLi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
